# üöÄ QUICKSTART - B·∫Øt ƒë·∫ßu trong 5 ph√∫t

## B∆∞·ªõc 1: C√†i ƒë·∫∑t

```bash
# Copy package v√†o project c·ªßa b·∫°n
cd /path/to/your/project
cp -r web_crawler .

# C√†i ƒë·∫∑t dependencies
pip install -r web_crawler/requirements.txt
```

## B∆∞·ªõc 2: S·ª≠ d·ª•ng ngay

### Option A: Crawl ƒë∆°n gi·∫£n nh·∫•t

```python
from web_crawler import WebCrawler

# Danh s√°ch URLs
urls = [
    "https://example.com",
    "https://python.org",
]

# T·∫°o crawler v√† ch·∫°y
crawler = WebCrawler(urls=urls, use_proxy=False)
stats = crawler.crawl()

print(f"‚úì Crawled {stats['success']} pages successfully!")
print(f"‚úì Results saved to: crawl_results.json")
```

### Option B: V·ªõi custom parser

```python
from web_crawler import WebCrawler
from bs4 import BeautifulSoup

def my_parser(url: str, html: str) -> dict:
    soup = BeautifulSoup(html, 'html.parser')
    return {
        'title': soup.title.string if soup.title else '',
        'links': [a['href'] for a in soup.find_all('a', href=True)[:10]]
    }

urls = ["https://example.com"]
crawler = WebCrawler(urls=urls, parser=my_parser)
crawler.crawl()
```

### Option C: L∆∞u t·ª´ng file ri√™ng

```python
from web_crawler import WebCrawler, PerURLStorage

urls = ["https://example.com", "https://python.org"]

storage = PerURLStorage(output_dir="my_results")
crawler = WebCrawler(urls=urls, storage=storage)
crawler.crawl()

# Results in: my_results/
```

### Option D: L∆∞u v√†o MongoDB

```python
from web_crawler import WebCrawler, MongoDBStorage

# Thay connection string c·ªßa b·∫°n
MONGODB_URI = "mongodb+srv://user:pass@cluster.mongodb.net/"

storage = MongoDBStorage(
    connection_string=MONGODB_URI,
    database="my_db",
    collection="crawled_data"
)

urls = ["https://example.com"]
crawler = WebCrawler(urls=urls, storage=storage)
crawler.crawl()
```

## B∆∞·ªõc 3: Ch·∫°y examples

```bash
cd web_crawler

# Example 1: Basic
python example_basic.py

# Example 2: Custom Parser
python example_custom_parser.py

# Example 3: Advanced
python example_advanced.py

# Quick Test
python test_quick.py

# Full Demo
python demo_comprehensive.py
```

## Common Tasks

### Task 1: Crawl v·ªõi proxy
```python
crawler = WebCrawler(
    urls=urls,
    use_proxy=True  # T·ª± ƒë·ªông l·∫•y proxy mi·ªÖn ph√≠
)
```

### Task 2: Crawl nhanh
```python
crawler = WebCrawler(
    urls=urls,
    max_workers=20,  # Nhi·ªÅu workers
    timeout=10,      # Timeout ng·∫Øn
    use_proxy=False
)
```

### Task 3: Crawl an to√†n
```python
crawler = WebCrawler(
    urls=urls,
    max_workers=3,   # √çt workers
    timeout=60,      # Timeout d√†i
    max_retries=5,   # Retry nhi·ªÅu
    use_proxy=True
)
```

## Configuration Reference

| Parameter | Default | M√¥ t·∫£ |
|-----------|---------|-------|
| `urls` | Required | List URLs c·∫ßn crawl |
| `parser` | Default | H√†m parse: `(url, html) -> dict` |
| `storage` | Aggregated | Storage backend |
| `max_workers` | 8 | Concurrent workers |
| `use_proxy` | True | D√πng proxy hay kh√¥ng |
| `timeout` | 30 | Timeout (seconds) |
| `max_retries` | 3 | S·ªë l·∫ßn retry |
| `retry_delay` | 2 | Delay gi·ªØa retries |

## Troubleshooting

**Problem: B·ªã timeout**
```python
crawler = WebCrawler(urls=urls, timeout=60)
```

**Problem: B·ªã block IP**
```python
crawler = WebCrawler(urls=urls, use_proxy=True)
```

**Problem: Parser l·ªói**
```python
def safe_parser(url, html):
    try:
        # your logic
        return {...}
    except Exception as e:
        return {'error': str(e)}
```

## ƒê·ªçc th√™m

- **README.md**: Overview v√† API reference
- **USAGE_GUIDE.md**: H∆∞·ªõng d·∫´n chi ti·∫øt v√† use cases
- **PROJECT_STRUCTURE.md**: C·∫•u tr√∫c project
- **examples/*.py**: Example scripts

## Support

C√≥ v·∫•n ƒë·ªÅ? Check:
1. README.md
2. USAGE_GUIDE.md  
3. Examples trong th∆∞ m·ª•c web_crawler/

---

**Happy Crawling! üï∑Ô∏è**
