# Web Crawler - HÆ°á»›ng dáº«n sá»­ dá»¥ng chi tiáº¿t

## Má»¥c lá»¥c

1. [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
2. [Quickstart](#quickstart)
3. [CÃ¡c tÃ­nh nÄƒng chÃ­nh](#cÃ¡c-tÃ­nh-nÄƒng-chÃ­nh)
4. [Use Cases thá»±c táº¿](#use-cases-thá»±c-táº¿)
5. [Troubleshooting](#troubleshooting)
6. [Performance Tuning](#performance-tuning)

## CÃ i Ä‘áº·t

### CÃ¡ch 1: Copy package vÃ o project

```bash
cp -r web_crawler /path/to/your/project/
cd /path/to/your/project
pip install -r web_crawler/requirements.txt
```

### CÃ¡ch 2: Install nhÆ° package

```bash
cd web_crawler
pip install -e .  # Install á»Ÿ cháº¿ Ä‘á»™ development
```

### CÃ¡ch 3: Install tá»« GitHub (náº¿u Ä‘Ã£ push lÃªn)

```bash
pip install git+https://github.com/yourusername/web-crawler.git
```

## Quickstart

### 5 phÃºt Ä‘á»ƒ báº¯t Ä‘áº§u

```python
from web_crawler import WebCrawler

# Táº¡o danh sÃ¡ch URLs
urls = ["https://example.com", "https://python.org"]

# Táº¡o crawler vÃ  cháº¡y
crawler = WebCrawler(urls=urls, use_proxy=False)
stats = crawler.crawl()

print(f"Crawled {stats['success']} pages in {stats['duration']}s")
```

Káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u trong file `crawl_results.json`.

## CÃ¡c tÃ­nh nÄƒng chÃ­nh

### 1. Custom Parser

Parser lÃ  hÃ m nháº­n `(url, html)` vÃ  tráº£ vá» `dict` chá»©a data báº¡n muá»‘n extract.

#### Example: E-commerce Product Scraper

```python
from bs4 import BeautifulSoup
from web_crawler import WebCrawler

def product_parser(url: str, html: str) -> dict:
    """Extract thÃ´ng tin sáº£n pháº©m"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # TÃ¬m thÃ´ng tin sáº£n pháº©m (tÃ¹y thuá»™c vÃ o cáº¥u trÃºc HTML)
    product = {}
    
    # Title
    title_elem = soup.find('h1', class_='product-title')
    product['title'] = title_elem.get_text().strip() if title_elem else ''
    
    # Price
    price_elem = soup.find('span', class_='price')
    product['price'] = price_elem.get_text().strip() if price_elem else ''
    
    # Description
    desc_elem = soup.find('div', class_='description')
    product['description'] = desc_elem.get_text().strip() if desc_elem else ''
    
    # Images
    images = [img['src'] for img in soup.find_all('img', class_='product-image')]
    product['images'] = images
    
    # Rating
    rating_elem = soup.find('span', class_='rating')
    product['rating'] = rating_elem.get_text().strip() if rating_elem else ''
    
    return product

# Sá»­ dá»¥ng
urls = [
    "https://example-shop.com/product/1",
    "https://example-shop.com/product/2",
]

crawler = WebCrawler(urls=urls, parser=product_parser)
crawler.crawl()
```

#### Example: News Article Scraper

```python
def article_parser(url: str, html: str) -> dict:
    """Extract bÃ i bÃ¡o"""
    soup = BeautifulSoup(html, 'html.parser')
    
    article = {}
    
    # Title
    article['title'] = soup.find('h1').get_text().strip()
    
    # Author
    author_elem = soup.find('span', class_='author')
    article['author'] = author_elem.get_text().strip() if author_elem else ''
    
    # Date
    date_elem = soup.find('time')
    article['date'] = date_elem['datetime'] if date_elem else ''
    
    # Content (láº¥y táº¥t cáº£ paragraphs)
    paragraphs = soup.find_all('p', class_='article-content')
    article['content'] = '\n\n'.join([p.get_text().strip() for p in paragraphs])
    
    # Tags
    tags = [tag.get_text().strip() for tag in soup.find_all('a', class_='tag')]
    article['tags'] = tags
    
    return article
```

### 2. Storage Backends

#### PerURLStorage - LÆ°u tá»«ng file riÃªng

Tá»‘t cho: Crawl nhiá»u URLs, cáº§n xem tá»«ng káº¿t quáº£ riÃªng láº»

```python
from web_crawler import WebCrawler, PerURLStorage

storage = PerURLStorage(output_dir="scraped_products")
crawler = WebCrawler(urls=urls, storage=storage)
crawler.crawl()
```

Káº¿t quáº£:
```
scraped_products/
  â”œâ”€â”€ example.com_product1_abc123.json
  â”œâ”€â”€ example.com_product2_def456.json
  â””â”€â”€ ...
```

#### AggregatedStorage - LÆ°u táº¥t cáº£ vÃ o 1 file

Tá»‘t cho: PhÃ¢n tÃ­ch tá»•ng thá»ƒ, export dá»… dÃ ng

```python
from web_crawler import AggregatedStorage

storage = AggregatedStorage(output_file="all_products.json")
crawler = WebCrawler(urls=urls, storage=storage)
crawler.crawl()
```

#### MongoDBStorage - LÆ°u vÃ o database

Tá»‘t cho: Production, cáº§n query/filter, scale lá»›n

```python
from web_crawler import MongoDBStorage

# MongoDB Atlas connection string
MONGODB_URI = "mongodb+srv://username:password@cluster.mongodb.net/"

storage = MongoDBStorage(
    connection_string=MONGODB_URI,
    database="ecommerce",
    collection="products"
)

crawler = WebCrawler(urls=urls, storage=storage)
crawler.crawl()
```

### 3. Proxy Management

#### Sá»­ dá»¥ng proxy tá»± Ä‘á»™ng

```python
crawler = WebCrawler(
    urls=urls,
    use_proxy=True  # Tá»± Ä‘á»™ng láº¥y proxy miá»…n phÃ­
)
crawler.crawl()
```

#### ThÃªm proxy cá»§a báº¡n

```python
crawler = WebCrawler(urls=urls, use_proxy=True)

# ThÃªm proxy riÃªng (tá»‘t hÆ¡n proxy miá»…n phÃ­)
crawler.proxy_manager.add_proxies([
    "http://proxy1.yourservice.com:8080",
    "http://proxy2.yourservice.com:8080",
    "socks5://proxy3.yourservice.com:1080"
])

crawler.crawl()
```

#### Custom proxy sources

```python
from web_crawler import ProxyManager

custom_sources = [
    "https://your-proxy-api.com/list",
    "https://another-source.com/proxies"
]

crawler = WebCrawler(
    urls=urls,
    use_proxy=True,
    proxy_sources=custom_sources
)
```

### 4. Performance Tuning

#### Crawl nhanh (nhiá»u URLs)

```python
crawler = WebCrawler(
    urls=urls,
    max_workers=20,    # Nhiá»u workers
    timeout=10,        # Timeout ngáº¯n
    max_retries=2,     # Ãt retries
    use_proxy=False    # KhÃ´ng dÃ¹ng proxy cho nhanh
)
```

#### Crawl an toÃ n (Ã­t URLs, quan trá»ng)

```python
crawler = WebCrawler(
    urls=urls,
    max_workers=3,     # Ãt workers
    timeout=60,        # Timeout dÃ i
    max_retries=5,     # Nhiá»u retries
    retry_delay=3,     # Delay dÃ i hÆ¡n
    use_proxy=True     # DÃ¹ng proxy Ä‘á»ƒ trÃ¡nh block
)
```

## Use Cases thá»±c táº¿

### Case 1: Crawl danh sÃ¡ch sáº£n pháº©m tá»« e-commerce

```python
from web_crawler import WebCrawler, AggregatedStorage
from bs4 import BeautifulSoup

def product_list_parser(url: str, html: str) -> dict:
    """Parse trang danh sÃ¡ch sáº£n pháº©m"""
    soup = BeautifulSoup(html, 'html.parser')
    
    products = []
    for item in soup.find_all('div', class_='product-item'):
        product = {
            'name': item.find('h3').get_text().strip(),
            'price': item.find('span', class_='price').get_text().strip(),
            'url': item.find('a')['href']
        }
        products.append(product)
    
    return {'products': products, 'count': len(products)}

# URLs cá»§a cÃ¡c trang category
urls = [
    "https://shop.com/category/electronics?page=1",
    "https://shop.com/category/electronics?page=2",
    "https://shop.com/category/electronics?page=3",
]

crawler = WebCrawler(
    urls=urls,
    parser=product_list_parser,
    storage=AggregatedStorage("products.json"),
    max_workers=5
)

stats = crawler.crawl()
print(f"Crawled {stats['success']} pages")
```

### Case 2: Monitor giÃ¡ cáº£ (cháº¡y Ä‘á»‹nh ká»³)

```python
import time
from datetime import datetime
from web_crawler import WebCrawler, MongoDBStorage

def price_monitor_parser(url: str, html: str) -> dict:
    soup = BeautifulSoup(html, 'html.parser')
    
    return {
        'product_name': soup.find('h1').get_text().strip(),
        'current_price': soup.find('span', class_='price').get_text().strip(),
        'in_stock': 'in-stock' in html.lower(),
        'check_time': datetime.now().isoformat()
    }

# MongoDB Ä‘á»ƒ lÆ°u lá»‹ch sá»­ giÃ¡
storage = MongoDBStorage(
    connection_string="mongodb+srv://...",
    database="price_monitoring",
    collection="prices"
)

# URLs sáº£n pháº©m cáº§n monitor
urls = [
    "https://shop.com/product/laptop-gaming",
    "https://shop.com/product/iphone-15",
]

# Cháº¡y má»—i 1 giá»
while True:
    print(f"Checking prices at {datetime.now()}")
    
    crawler = WebCrawler(
        urls=urls,
        parser=price_monitor_parser,
        storage=storage,
        max_workers=2
    )
    crawler.crawl()
    
    print("Sleeping for 1 hour...")
    time.sleep(3600)  # 1 hour
```

### Case 3: Crawl tin tá»©c vÃ  phÃ¢n tÃ­ch

```python
from web_crawler import WebCrawler, PerURLStorage

def news_parser(url: str, html: str) -> dict:
    soup = BeautifulSoup(html, 'html.parser')
    
    # Extract article
    article = {
        'title': soup.find('h1').get_text().strip(),
        'content': ' '.join([p.get_text() for p in soup.find_all('p')]),
    }
    
    # Simple sentiment analysis (word counting)
    content_lower = article['content'].lower()
    positive_words = ['tá»‘t', 'tÄƒng', 'thÃ nh cÃ´ng', 'kháº£ quan']
    negative_words = ['giáº£m', 'kÃ©m', 'tháº¥t báº¡i', 'tá»“i']
    
    article['positive_score'] = sum(content_lower.count(w) for w in positive_words)
    article['negative_score'] = sum(content_lower.count(w) for w in negative_words)
    
    return article

# Crawl tin tá»©c vá» má»™t cÃ´ng ty
urls = [
    "https://news.com/article/company-q4-results",
    "https://news.com/article/company-new-product",
]

crawler = WebCrawler(
    urls=urls,
    parser=news_parser,
    storage=PerURLStorage("news_analysis")
)

crawler.crawl()
```

## Troubleshooting

### Problem 1: Bá»‹ block IP

**Giáº£i phÃ¡p:**
```python
# Sá»­ dá»¥ng proxy
crawler = WebCrawler(urls=urls, use_proxy=True)

# Giáº£m sá»‘ workers
crawler = WebCrawler(urls=urls, max_workers=2)

# ThÃªm delay
crawler = WebCrawler(urls=urls, retry_delay=5)
```

### Problem 2: Timeout quÃ¡ nhiá»u

**Giáº£i phÃ¡p:**
```python
# TÄƒng timeout
crawler = WebCrawler(urls=urls, timeout=60)

# TÄƒng retries
crawler = WebCrawler(urls=urls, max_retries=5)
```

### Problem 3: Parser bá»‹ lá»—i

**Giáº£i phÃ¡p:**
```python
def safe_parser(url: str, html: str) -> dict:
    try:
        soup = BeautifulSoup(html, 'html.parser')
        # Your logic here
        return {...}
    except Exception as e:
        logging.error(f"Parse error for {url}: {e}")
        return {'error': str(e), 'url': url}
```

### Problem 4: Proxy khÃ´ng hoáº¡t Ä‘á»™ng

**Giáº£i phÃ¡p:**
```python
# Táº¯t proxy náº¿u khÃ´ng cáº§n thiáº¿t
crawler = WebCrawler(urls=urls, use_proxy=False)

# Hoáº·c thÃªm proxy tá»‘t hÆ¡n
crawler = WebCrawler(urls=urls, use_proxy=True)
crawler.proxy_manager.add_proxies([
    "http://premium-proxy.com:8080"
])
```

## Performance Tuning

### Benchmark

Test vá»›i 100 URLs:

| Config | Time | Success Rate |
|--------|------|--------------|
| 5 workers, no proxy | 45s | 98% |
| 10 workers, no proxy | 28s | 95% |
| 20 workers, no proxy | 18s | 90% |
| 5 workers, with proxy | 120s | 85% |

**Káº¿t luáº­n:**
- KhÃ´ng dÃ¹ng proxy nhanh hÆ¡n nhiá»u nhÆ°ng dá»… bá»‹ block
- TÄƒng workers cáº£i thiá»‡n tá»‘c Ä‘á»™ nhÆ°ng giáº£m success rate
- DÃ¹ng proxy cháº­m hÆ¡n nhÆ°ng an toÃ n hÆ¡n

### Recommended Configs

**For development/testing:**
```python
crawler = WebCrawler(
    urls=urls,
    max_workers=5,
    use_proxy=False,
    timeout=15
)
```

**For production (small scale):**
```python
crawler = WebCrawler(
    urls=urls,
    max_workers=8,
    use_proxy=True,
    timeout=30,
    max_retries=3
)
```

**For production (large scale):**
```python
crawler = WebCrawler(
    urls=urls,
    max_workers=15,
    use_proxy=True,
    timeout=20,
    max_retries=2,
    storage=MongoDBStorage(...)
)
```

## Best Practices

1. **Respect robots.txt**: Kiá»ƒm tra vÃ  tuÃ¢n thá»§ robots.txt cá»§a website
2. **Rate limiting**: KhÃ´ng crawl quÃ¡ nhanh, cÃ³ thá»ƒ lÃ m crash website
3. **Error handling**: LuÃ´n cÃ³ error handling trong parser
4. **Logging**: Enable logging Ä‘á»ƒ debug
5. **Testing**: Test parser vá»›i Ã­t URLs trÆ°á»›c khi cháº¡y full
6. **Storage**: Chá»n storage phÃ¹ há»£p vá»›i use case
7. **Proxy**: DÃ¹ng proxy cÃ³ cháº¥t lÆ°á»£ng cho production
8. **Monitoring**: Monitor success rate vÃ  Ä‘iá»u chá»‰nh config

## Advanced Topics

### Custom Storage Backend

```python
from web_crawler.storage import StorageBackend

class CustomStorage(StorageBackend):
    async def save(self, url: str, data: dict):
        # Your custom save logic
        pass
    
    async def finalize(self):
        # Cleanup logic
        pass

storage = CustomStorage()
crawler = WebCrawler(urls=urls, storage=storage)
```

### Middleware Pattern

```python
def middleware_parser(url: str, html: str) -> dict:
    """Parser with middleware logic"""
    
    # Pre-processing
    html = html.replace('&nbsp;', ' ')
    
    # Main parsing
    soup = BeautifulSoup(html, 'html.parser')
    data = {...}
    
    # Post-processing
    data['url'] = url
    data['crawled_at'] = datetime.now().isoformat()
    
    return data
```

---

**Happy Crawling! ğŸ•·ï¸**
