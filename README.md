# Web Crawler Package

Package Python máº¡nh máº½ Ä‘á»ƒ crawl dá»¯ liá»‡u web vá»›i há»— trá»£ async, proxy rotation, vÃ  nhiá»u storage backends.

## ğŸŒŸ TÃ­nh nÄƒng

- âœ… **Async I/O** vá»›i aiohttp Ä‘á»ƒ crawl nhanh
- âœ… **Multi-threading** cÃ³ thá»ƒ Ä‘iá»u chá»‰nh (máº·c Ä‘á»‹nh 8 workers)
- âœ… **Proxy Rotation** tá»± Ä‘á»™ng láº¥y proxy tá»« cÃ¡c nguá»“n miá»…n phÃ­ + auto-refetch
- âœ… **Progress Bar** ğŸ“Š NEW - Thanh tiáº¿n Ä‘á»™ real-time vá»›i tqdm
- âœ… **Chain Crawling** â­ NEW - Crawl theo chuá»—i nhiá»u bÆ°á»›c (URL1 â†’ URL2 â†’ URL3)
- âœ… **Custom Parser** Ä‘á»ƒ xá»­ lÃ½ HTML theo Ã½ muá»‘n
- âœ… **3 Storage Backends**:
  - Per-URL Storage (lÆ°u tá»«ng file riÃªng)
  - Aggregated Storage (lÆ°u táº¥t cáº£ vÃ o 1 file)
  - MongoDB Atlas (lÆ°u vÃ o database)
- âœ… **Retry Logic** vá»›i backoff
- âœ… **Logging** chi tiáº¿t
- âœ… **Statistics** tracking

## ğŸ“¦ CÃ i Ä‘áº·t

```bash
# Clone hoáº·c copy package vÃ o project cá»§a báº¡n
cd your_project
cp -r web_crawler .

# CÃ i Ä‘áº·t dependencies
pip install -r web_crawler/requirements.txt
```

### Dependencies

```
aiohttp>=3.9.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
motor>=3.3.0  # Optional: chá»‰ cáº§n náº¿u dÃ¹ng MongoDB
```

## ğŸš€ Sá»­ dá»¥ng nhanh

### 1. Sá»­ dá»¥ng cÆ¡ báº£n

```python
from web_crawler import WebCrawler

urls = [
    "https://example.com",
    "https://python.org",
]

crawler = WebCrawler(urls=urls)
stats = crawler.crawl()

print(f"Success: {stats['success']}/{stats['total']}")
```

### 2. Custom Parser

```python
from web_crawler import WebCrawler, PerURLStorage
from bs4 import BeautifulSoup

def my_parser(url: str, html: str) -> dict:
    """Parse HTML vÃ  tráº£ vá» data báº¡n muá»‘n"""
    soup = BeautifulSoup(html, 'html.parser')
    
    return {
        'title': soup.title.string if soup.title else '',
        'headings': [h.get_text() for h in soup.find_all(['h1', 'h2'])],
        'links': [a['href'] for a in soup.find_all('a', href=True)]
    }

crawler = WebCrawler(
    urls=urls,
    parser=my_parser,
    storage=PerURLStorage(output_dir="my_results")
)

crawler.crawl()
```

### 3. LÆ°u vÃ o MongoDB

```python
from web_crawler import WebCrawler, MongoDBStorage

storage = MongoDBStorage(
    connection_string="mongodb+srv://user:pass@cluster.mongodb.net/",
    database="my_db",
    collection="crawled_data"
)

crawler = WebCrawler(
    urls=urls,
    storage=storage
)

crawler.crawl()
```

### 4. Chain Crawling â­ NEW

Crawl theo chuá»—i nhiá»u bÆ°á»›c (vÃ­ dá»¥: Category â†’ Products â†’ Details):

```python
from web_crawler import ChainCrawler, ChainStep
from bs4 import BeautifulSoup

# Step 1: Extract product URLs from category
def step1_parser(url, html):
    soup = BeautifulSoup(html, 'html.parser')
    return {'links': [a['href'] for a in soup.find_all('a', class_='product')]}

def step1_extract(data):
    return data['links']

# Step 2: Extract final data from products
def step2_parser(url, html):
    soup = BeautifulSoup(html, 'html.parser')
    return {
        'title': soup.find('h1').get_text(),
        'price': soup.find('span', class_='price').get_text()
    }

# Define chain
steps = [
    ChainStep("Get Products", step1_parser, step1_extract),
    ChainStep("Parse Products", step2_parser, None)  # Final step
]

# Run chain
crawler = ChainCrawler(
    initial_urls=["https://shop.com/category/electronics"],
    steps=steps
)
crawler.crawl()
```

## ğŸ“š Chi tiáº¿t API

### WebCrawler

Constructor parameters:

| Parameter | Type | Default | MÃ´ táº£ |
|-----------|------|---------|-------|
| `urls` | `List[str]` | Required | Danh sÃ¡ch URLs cáº§n crawl |
| `parser` | `Callable` | Default parser | HÃ m parse HTML: `(url, html) -> data` |
| `storage` | `StorageBackend` | `AggregatedStorage()` | Backend lÆ°u trá»¯ |
| `max_workers` | `int` | `8` | Sá»‘ concurrent workers |
| `use_proxy` | `bool` | `True` | Sá»­ dá»¥ng proxy hay khÃ´ng |
| `proxy_sources` | `List[str]` | Default sources | Custom proxy sources |
| `timeout` | `int` | `30` | Timeout má»—i request (seconds) |
| `max_retries` | `int` | `3` | Sá»‘ láº§n retry khi fail |
| `retry_delay` | `int` | `2` | Delay giá»¯a cÃ¡c retry (seconds) |

Methods:

```python
crawler.crawl() -> dict  # Báº¯t Ä‘áº§u crawl, tráº£ vá» statistics
crawler.add_urls(urls: List[str])  # ThÃªm URLs vÃ o danh sÃ¡ch
```

### Storage Backends

#### PerURLStorage

LÆ°u má»—i URL thÃ nh má»™t file JSON riÃªng.

```python
from web_crawler import PerURLStorage

storage = PerURLStorage(output_dir="results")
```

Output structure:
```
results/
  â”œâ”€â”€ example.com_abc123.json
  â”œâ”€â”€ python.org_def456.json
  â””â”€â”€ ...
```

#### AggregatedStorage

LÆ°u táº¥t cáº£ káº¿t quáº£ vÃ o má»™t file JSON.

```python
from web_crawler import AggregatedStorage

storage = AggregatedStorage(output_file="all_results.json")
```

Output structure:
```json
[
  {
    "url": "https://example.com",
    "timestamp": "2024-01-01T12:00:00",
    "data": { ... }
  },
  ...
]
```

#### MongoDBStorage

LÆ°u vÃ o MongoDB Atlas (hoáº·c MongoDB báº¥t ká»³).

```python
from web_crawler import MongoDBStorage

storage = MongoDBStorage(
    connection_string="mongodb+srv://...",
    database="web_crawler",
    collection="results"
)
```

### ProxyManager

Quáº£n lÃ½ proxy tá»± Ä‘á»™ng.

```python
from web_crawler import ProxyManager

proxy_manager = ProxyManager(
    custom_sources=[
        "https://api.proxyscrape.com/...",
        "https://custom-proxy-source.com/..."
    ]
)

# Láº¥y proxy
await proxy_manager.fetch_proxies()

# Get random proxy
proxy = proxy_manager.get_proxy()

# Mark proxy as failed
proxy_manager.mark_failed(proxy)

# Add manual proxies
proxy_manager.add_proxies([
    "http://proxy1.com:8080",
    "http://proxy2.com:8080"
])
```

## ğŸ¯ Examples

Package Ä‘i kÃ¨m vá»›i 4 example files:

1. **example_basic.py** - Sá»­ dá»¥ng cÆ¡ báº£n
2. **example_custom_parser.py** - Custom parser Ä‘á»ƒ extract dá»¯ liá»‡u cá»¥ thá»ƒ
3. **example_mongodb.py** - LÆ°u vÃ o MongoDB
4. **example_advanced.py** - Káº¿t há»£p táº¥t cáº£ tÃ­nh nÄƒng

Cháº¡y examples:

```bash
cd web_crawler
python example_basic.py
python example_custom_parser.py
python example_advanced.py
```

## ğŸ“Š Statistics

Má»—i láº§n crawl tráº£ vá» statistics:

```python
{
    'total': 100,          # Tá»•ng sá»‘ URLs
    'success': 95,         # Sá»‘ URLs crawl thÃ nh cÃ´ng
    'failed': 5,           # Sá»‘ URLs tháº¥t báº¡i
    'start_time': 1234,    # Timestamp báº¯t Ä‘áº§u
    'end_time': 1256,      # Timestamp káº¿t thÃºc
    'duration': 22.5       # Thá»i gian (seconds)
}
```

## ğŸ”§ Cáº¥u hÃ¬nh Proxy

Package tá»± Ä‘á»™ng láº¥y proxy tá»« cÃ¡c nguá»“n miá»…n phÃ­:

- ProxyScrape API
- CÃ³ thá»ƒ thÃªm nguá»“n custom

Hoáº·c thÃªm proxy thá»§ cÃ´ng:

```python
crawler = WebCrawler(urls=urls, use_proxy=True)

# ThÃªm proxy cá»§a báº¡n
crawler.proxy_manager.add_proxies([
    "http://your-proxy-1.com:8080",
    "http://your-proxy-2.com:8080",
    "socks5://your-proxy-3.com:1080"
])

crawler.crawl()
```

## ğŸ“ Logging

Enable logging Ä‘á»ƒ theo dÃµi quÃ¡ trÃ¬nh crawl:

```python
import logging

logging.basicConfig(
    level=logging.INFO,  # hoáº·c DEBUG Ä‘á»ƒ xem chi tiáº¿t hÆ¡n
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

## âš™ï¸ Best Practices

### 1. Tuning Performance

```python
# Cho nhiá»u URLs (1000+)
crawler = WebCrawler(
    urls=urls,
    max_workers=20,  # TÄƒng workers
    timeout=15,      # Giáº£m timeout
    max_retries=2    # Giáº£m retries
)

# Cho Ã­t URLs nhÆ°ng cáº§n chÃ­nh xÃ¡c
crawler = WebCrawler(
    urls=urls,
    max_workers=5,   # Ãt workers hÆ¡n
    timeout=60,      # Timeout dÃ i hÆ¡n
    max_retries=5    # Nhiá»u retries hÆ¡n
)
```

### 2. Custom Parser Tips

```python
def efficient_parser(url: str, html: str) -> dict:
    """Parser hiá»‡u quáº£"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # Chá»‰ extract nhá»¯ng gÃ¬ cáº§n thiáº¿t
    data = {
        'title': soup.title.string if soup.title else '',
    }
    
    # Sá»­ dá»¥ng CSS selectors Ä‘á»ƒ nhanh hÆ¡n
    products = soup.select('.product-item')
    data['products'] = [p.get_text() for p in products[:10]]
    
    return data
```

### 3. Error Handling

```python
def safe_parser(url: str, html: str) -> dict:
    """Parser vá»›i error handling"""
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # Your parsing logic
        return {'title': soup.title.string}
        
    except Exception as e:
        logging.error(f"Parse error for {url}: {e}")
        return {'error': str(e)}
```

## ğŸ”’ LÆ°u Ã½

- **Proxy miá»…n phÃ­** cÃ³ thá»ƒ khÃ´ng á»•n Ä‘á»‹nh, nÃªn chuáº©n bá»‹ proxy riÃªng cho production
- **Respect robots.txt** vÃ  terms of service cá»§a cÃ¡c websites
- **Rate limiting**: Äiá»u chá»‰nh `max_workers` phÃ¹ há»£p Ä‘á»ƒ trÃ¡nh overload
- **MongoDB**: Cáº§n cÃ i `motor` package riÃªng

## ğŸ“„ License

MIT License - Free to use and modify

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Support

Náº¿u cÃ³ váº¥n Ä‘á», hÃ£y táº¡o issue hoáº·c liÃªn há»‡ support.
